{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Structure and Geometry of the Human Brain\n",
    "\n",
    "[Noah C. Benson](https://nben.net/) &lt;[nben@uw.edu](mailto:nben@uw.edu)&gt;  \n",
    "[eScience Institute](https://escience.washingtonn.edu/)  \n",
    "[University of Washington](https://www.washington.edu/)  \n",
    "[Seattle, WA 98195](https://seattle.gov/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed to accompany the lecture \"Introduction to the Strugure and Geometry of the Human Brain\" as part of the Neurohackademt 2020 curriculum. It can be run either in Neurohackademy's Jupyterhub environment, or using the `docker-compose.yml` file (see the `README.md` file for instructions).\n",
    "\n",
    "In this notebook we will examine various structural and geometric data used commonly in neuroscience. These demos will primarily use [FreeSurfer](http://surfer.nmr.mgh.harvard.edu/) subjects. In the lecture and the Neurohackademy Jupyterhub environment, we will look primarily at a subject named `nben`; however, you can alternately use the subject `bert`, which is an example subject that comes with FreeSurfer. Optionally, this notebook can be used with subject from the [Human Connectome Project (HCP)](https://db.humanconnectome.org/)--see the `README.md` file for instructions on getting credentials for use with the HCP.\n",
    "\n",
    "We will look at these data using both the [`nibabel`](https://nipy.org/nibabel/), which is an excellent core library for importing various kinds of neuroimaging data, as well as [`neuropythy`](https://github.com/noahbenson/neuropythy), which builds on `nibabel` to provide a user-friendly API for interacting with subjects. At its core, `neuropythy` is a library for interacting with neuroscientific data in the context of brain structure.\n",
    "\n",
    "This notebook itself consists of this introduction as well as four sections that follow the topic areas in the slide-deck from the lecture. These sections are intended to be explored in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running any of the code in this notebook, we need to start by importing a few libraries and making sure we have configured those that need to be configured (mainly, `matplotlib`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need os for paths:\n",
    "import os\n",
    "# Numpy, Scipy, and Matplotlib are effectively standard libraries.\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "# Ipyvolume is a 3D plotting library that is used by neuropythy.\n",
    "import ipyvolume  as ipv\n",
    "# Nibabel is the library that understands various neuroimaging file\n",
    "# formats; it is also used by neuropythy.\n",
    "import nibabel as nib\n",
    "\n",
    "# Neuropythy is the main library we will be using in this notebook.\n",
    "import neuropythy as ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## MRI and Volumetric Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first section of this notebook will deal with MR images and volumetric data. We will start by loading in an MRImage. We will use the same image that was visualized in the lecture (if you are not using the Jupyterhub, you won't have access to this subject, but you can use the subject `'bert'` instead).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Load a subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "For starters, we will load the subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_id = 'nben'\n",
    "\n",
    "subject = ny.freesurfer_subject(subject_id)\n",
    "\n",
    "# If you have configured the HCP credentials and wish to use an HCP\n",
    "# subject instead of nben:\n",
    "#\n",
    "#subject_id = 111312\n",
    "#subject = ny.hcp_subject(subject_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `freesurfer_subject` function returns a `neuropythy` `Subject` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Load an MRImage file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Let's load in an image file. FreeSurfer directories contain a subdirectory `mri/` that contains all of the volumetric/image data for the subject. This includes images that have been preprocessed as well as copies of the original T1-weighted image. We will load an image called `T1.mgz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will load data from a subject's directory using neuropythy's\n",
    "# builtin ny.load() function; in most cases, this calls down to nibabel's own\n",
    "# nib.load() function.\n",
    "im = subject.load('mri/T1.mgz')\n",
    "\n",
    "# For an HCP subject, use this file instead:\n",
    "#im = subject.load(\"T1w/T1w_acpc_dc.nii.gz\")\n",
    "\n",
    "# The return value should be a nibabel image object.\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In fact, we could just as easily have loaded the same object using nibabel:\n",
    "im_from_nibabel = nib.load(subject.path + '/mri/T1.mgz')\n",
    "print('From neuropythy: ', im.get_filename())\n",
    "print('From nibabel:    ', im_from_nibabel.get_filename())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And neuropythy manages this image as part of the subject-data. Neuropythy's\n",
    "# name for it is 'intensity_normalized', which is due to its position as an \n",
    "# output in FreeSurfer's processing pipeline.\n",
    "subject.images['intensity_normalized'].get_filename()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Visualize some slices of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next, we will make 2D plots of some of the image slices. Feel free to change which slices you visualize; I have just chosen some defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What axis do we want to plot slices along? 0, 1, or 2 (for the first, second,\n",
    "# or third 3D image axis).\n",
    "axis = 2\n",
    "# Which slices along this axis should we plot? These must be at least 0 and at\n",
    "# most 255 (There are 256 slices in each dimension of these images).\n",
    "slices = [75, 125, 175]\n",
    "\n",
    "# Make a figure and axes using matplotlib.pyplot:\n",
    "(fig, axes) = plt.subplots(1, len(slices), figsize=(5, 5/len(slices)), dpi=144)\n",
    "# Plot each of the slices:\n",
    "for (ax, slice_num) in zip(axes, slices):\n",
    "    # Get the slice:\n",
    "    if axis == 0:\n",
    "        imslice = im.dataobj[slice_num,:,:]\n",
    "    elif axis == 1:\n",
    "        imslice = im.dataobj[:,slice_num,:]\n",
    "    else:\n",
    "        imslice = im.dataobj[:,:,slice_num]\n",
    "    ax.imshow(imslice, cmap='gray')\n",
    "    # Turn off labels:\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Visualize the 3D Image as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Next we will use `ipyvolume` to render a 3D View of the volume. The volume plotting function is part of `ipyvolume` and has a variety of options that are beyond the scope of this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this will generate a warning, which can be safely ignored.\n",
    "ipv.quickvolshow(sub.images['intensity_normalized'].dataobj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Load and visualize anatomical segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "FreeSurfer creates a segmentation image file called `aseg.mgz`, which we can load and use to identify ROIs. First, we will load this file and plot some slices from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First load the file; any of these lines will work:\n",
    "#aseg = subject.load('mri/aseg.mgz')\n",
    "#aseg = nib.load(subject.path + '/mri/aseg.mgz')\n",
    "aseg = subject.images['segmentation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot this as-is, but we don't know what the values in the numbers correspond to. Nonetheless, let's go ahead. This code block is the same as the block we used to plot slices above except that it uses the new image `aseg` we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What axis do we want to plot slices along? 0, 1, or 2 (for the first, second,\n",
    "# or third 3D image axis).\n",
    "axis = 2\n",
    "# Which slices along this axis should we plot? These must be at least 0 and at\n",
    "# most 255 (There are 256 slices in each dimension of these images).\n",
    "slices = [75, 125, 175]\n",
    "\n",
    "# Make a figure and axes using matplotlib.pyplot:\n",
    "(fig, axes) = plt.subplots(1, len(slices), figsize=(5, 5/len(slices)), dpi=144)\n",
    "# Plot each of the slices:\n",
    "for (ax, slice_num) in zip(axes, slices):\n",
    "    # Get the slice:\n",
    "    if axis == 0:\n",
    "        imslice = aseg.dataobj[slice_num,:,:]\n",
    "    elif axis == 1:\n",
    "        imslice = aseg.dataobj[:,slice_num,:]\n",
    "    else:\n",
    "        imslice = aseg.dataobj[:,:,slice_num]\n",
    "    ax.imshow(imslice, cmap='gray')\n",
    "    # Turn off labels:\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the balues in the plots above are discretized, but it's not clear what they correspond to. The map of numbers to characters and colors can be found in the various FreeSurfer color LUT files. These are all located in the FreeSurfer home directory and end with `LUT.txt`. They are essentially spreadsheets and are loaded by `neuropythy` as `pandas.DataFrame` objects. In `neuropythy`, the LUT objects are associated with the `'freesurfer_home'` configuration variable. This has been setup automatically in the course and the `neuropythy` docker-image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny.config['freesurfer_home'].luts['aseg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So suppose we want to look at left cerebral cortex. In the table, this has value 3. We can find this value in the images we are plotting and plot only it to see the ROI in each the slices we plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to plot left cerebral cortex (label ID = 3, per the LUT)\n",
    "label = 3\n",
    "\n",
    "(fig, axes) = plt.subplots(1, len(slices), figsize=(5, 5/len(slices)), dpi=144)\n",
    "# Plot each of the slices:\n",
    "for (ax, slice_num) in zip(axes, slices):\n",
    "    # Get the slice:\n",
    "    if axis == 0:\n",
    "        imslice = aseg.dataobj[slice_num,:,:]\n",
    "    elif axis == 1:\n",
    "        imslice = aseg.dataobj[:,slice_num,:]\n",
    "    else:\n",
    "        imslice = aseg.dataobj[:,:,slice_num]\n",
    "    # Plot only the values that are equal to the label ID.\n",
    "    imslice = (imslice == label)\n",
    "    ax.imshow(imslice, cmap='gray')\n",
    "    # Turn off labels:\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the LH cortex specifically, we can see that LEFT is in the direction of increasing rows (down the image slices, if you used `axis = 2`), thus RIGHT must be in the direction of decreasing rows in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also make some images from these slices in which we replace each of the pixels in each slice with the color recommended by the color LUT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using this color LUT:\n",
    "lut = ny.config['freesurfer_home'].luts['aseg']\n",
    "# The axis:\n",
    "axis = 2\n",
    "\n",
    "(fig, axes) = plt.subplots(1, len(slices), figsize=(5, 5/len(slices)), dpi=144)\n",
    "# Plot each of the slices:\n",
    "for (ax, slice_num) in zip(axes, slices):\n",
    "    # Get the slice:\n",
    "    if axis == 0:\n",
    "        imslice = aseg.dataobj[slice_num,:,:]\n",
    "    elif axis == 1:\n",
    "        imslice = aseg.dataobj[:,slice_num,:]\n",
    "    else:\n",
    "        imslice = aseg.dataobj[:,:,slice_num]\n",
    "    # Convert the slice into an RGBA image using the color LUT:\n",
    "    rgba_im = np.zeros(imslice.shape + (4,))\n",
    "    for (label_id, row) in lut.iterrows():\n",
    "        rgba_im[imslice == label_id,:] = row['color']\n",
    "    ax.imshow(rgba_im)\n",
    "    # Turn off labels:\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Cortical Surface Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cortical surface data is handled and represented much differently than volumetric data. This section demonstrates how to interact with cortical surface data in a Jupyter notebook, primarily using `neuropythy`.\n",
    "\n",
    "To start off, however, we will just load a surface file using `nibabel` to see what one contains.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Load a Surface-Geometry File Using `nibabel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each subject has a number of surface files; we will look at the\n",
    "# left hemisphere, white surface.\n",
    "hemi = 'lh'\n",
    "surf = 'white'\n",
    "# Feel free to change hemi to 'rh' for the RH and surf to 'pial'\n",
    "# or 'inflated'.\n",
    "\n",
    "# We load the surface from the subject's 'surf' directory in FreeSurfer.\n",
    "# Nibabel refers to these files as \"geometry\" files.\n",
    "filename = subject.path + f'/surf/{hemi}.{surf}'\n",
    "# If you are using an HCP subject, you should instead load from this path:\n",
    "#relpath = f'T1w/{subject.name}/surf/{hemi}.{surf}'\n",
    "#filename = subject.pseudo_path.local_path(relpath)\n",
    "\n",
    "# Read the file, using nibabel.\n",
    "surface_data = nib.freesurfer.read_geometry(filename)\n",
    "\n",
    "# What does this return?\n",
    "surface_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when `nibabel` reads in one of these surface files, what we get back is an `n x 3` matrix of real numbers (coordiantes) and an `m x 3` matrix of integers (triangle indices).\n",
    "\n",
    "The `ipyvolume` module has support for plotting triangle meshes--let's see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the coordinates and triangle-faces.\n",
    "(coords, faces) = surface_data\n",
    "# And get the (x,y,z) from coordinates.\n",
    "(x, y, z) = coords.T\n",
    "\n",
    "# Now, plot the triangle mesh.\n",
    "fig = ipv.plot_trisurf(x, y, z, triangles=faces)\n",
    "# Adjust the plot limits (making them equal makes the plot look good).\n",
    "ipv.pylab.xlim(-100,100)\n",
    "ipv.pylab.ylim(-100,100)\n",
    "ipv.pylab.zlim(-100,100)\n",
    "# Generally, one must call show() with ipyvolume.\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Hemisphere (`neuropythy.mri.Cortex`) objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Although one can load and plot cortical surfaces with `nibabel`, `neuropythy` builds on `nibabel` by providing a framework around which the cortical surface can be represented. It includes a number of utilities related specifically to cortical surface analysis, and allows much of the power of FreeSurfer to be leveraged through simple Python data structures.\n",
    "\n",
    "To start with, we will look at our subject's hemispheres (`neuropythy.mri.Cortex` objects) and how they represent surfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the hemisphere for our subject.\n",
    "cortex = subject.hemis[hemi]\n",
    "# Note that `cortex = subject.lh` and `cortex = subject.rh` are equivalent\n",
    "# to `cortex = subject.hemis['lh']` and `cortex = subject.hemis['rh']`.\n",
    "\n",
    "# What is cortex?\n",
    "cortex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see which hemisphere we have selected, the number of triangle faces that it has, and the number of vertices that it has. Let's look at a few of its' properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "#### Surfaces\n",
    "\n",
    "Each hemisphere has a number of surfaces; we can view them through the `cortex.surfaces` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex.surfaces.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cortex.surfaces['white_smooth']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `'white_smooth'` mesh is a well-processed mesh of the white surface that has been well-smoothed. You might notice that there is a `'midgray'` surface, even though FreeSurfer does not include a mid-gray mesh file. The `'midgray'` mesh, however, can be made by averaging the white and pial mesh vertices.\n",
    "\n",
    "Recall that all surfaces of a hemisphere have equivalent vertices and identical triangles. We can test that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(cortex.surfaces['white'].tess.faces,\n",
    "               cortex.surfaces['pial'].tess.faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surfaces track a large amount of data about their meshes and vertices and inherit most of the properties of hemispheres that are discussed below. In addition, surfaces uniquely carry data about cortical distances and surface areas. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The area of each of the triangle-faces in nthe white surface mesh, in mm^2.\n",
    "cortex.surfaces['white'].face_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The length of each edge in the white surface mesh, in mm.\n",
    "cortex.surfaces['white'].edge_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the edges themselves, as indices like the faces.\n",
    "cortex.surfaces['white'].tess.edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vertex Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties arre values assigned to each surface vertex. They can include anatomical or geometric properties, such as ROI labels (i.e., a vector of values for each vertex: `True` if the vertex is in the ROI and `False` if not), cortical thickness (in mm), the vertex surface-area (in square mm), the curvature, or data from other functional measurements, such as BOLD-time-series data or source-localized MEG data.\n",
    "\n",
    "The properties of a hemisphere are stored in the `properties` value. `Cortex.properties` is a kind of dictionary object and can generally be treated as a dictionary.  One can also access property vectors via `cortex.prop(property_name)` rather than `cortex.properties[property_name]`; the former is largely short-hand for the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(cortex.properties.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few thigs worth noting: First, not all FreeSurfer subjects will have all of the properties listed. This is because different versions of FreeSurfer include different files, and sometimes subjects are distributed without their full set of files (e.g., to save storage space). However, rather than go and try to load all of these files right away, `neuropythy` makes place-holders for them and loads them only when first requested (this saves on loading time drastically). Accordingly, if you try to use a property whose file doesn't exist, an nexception will be raised.\n",
    "\n",
    "Additionally, notice that the first several properties are for Brodmann Area labels. The ones ending in `_label` are `True` / `False` boolean labels indicating whether the vertex is in the given ROI (according to an estimation based on anatomy). The subject we are using in the Jupyterhub environment does not actually have these files included, but they do have, for example `BA1_weight` files. The weights represent the probability that a vertex is in the associated ROI, so we can make a label from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ba1_label = cortex.prop('BA1_weight') >= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot this property using `neuropythy`'s `cortex_plot()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny.cortex_plot(cortex.surfaces['white'], color=ba1_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Improving this plot.** While this plot shows us where the ROI is, it's rather hard to interpret. Rather, we would prefer to plot the ROI in red and the rest of the brain using a binarized curvature map. `neuropythy` supports this kind of binarized curvature map as a default underlay, so, in fact, the easiest way to accomplish this is to tell `cortex_plot` to color the surface red, but to add a vertex mask that instructs the function to *only* color the ROI vertices.\n",
    "\n",
    "Additionally, it is easier to see the inflated surface, so we will switch to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny.cortex_plot(cortex.surfaces['inflated'], color='r', mask=ba1_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optionally make this red ROI plot a little bit transparent as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny.cortex_plot(cortex.surfaces['inflated'], color='r', mask=ba1_label, alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting the weight instead of the label.** Alternately, we might have wanted to plot the weight / probability of the ROI. Continuous properties like probability can be plotted using color-maps, similar to how they are plotted in `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny.cortex_plot(cortex.surfaces['inflated'], color='BA1_weight',\n",
    "               cmap='hot', vmin=0, vmax=1, alpha=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another property.** Other properties can be very informative. For example, the cortical thickness property, which is stored in mm. This can tell us the parts of the brain that are thick or not thick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny.cortex_plot(cortex.surfaces['inflated'], color='thickness',\n",
    "               cmap='hot', vmin=2, vmax=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Interpolation (Surface to Image and Image to Surface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Hemisphere/Cortex objects also manage interpolation, both to/from image volumes as well as to/from the cortical surfaces of other subjects (we will demo interpolation between subjects in the last section). Here we will focus on the former: interpolation to and from images.\n",
    "\n",
    "**Cortex to Image Interpolation.**\n",
    "Because our subjects only have structural data and do not have functional data, we do not have anything handy to interpolate out of a volume onto a surface. So instead, we will start by innterpolating from the cortex into the volume. A good property for this is the subject's cortical thickness. Thickness is difficult to calculate in the volume, so if one wants thickness data in a volume, it would typically be calculated using surface meshes then projected back into the volume. We will do that now.\n",
    "\n",
    "Note that in order to create a new image, we have to provide the interpolation method with some information about how the image is oriented and shaped. This includes two critical pieces of information: the `'image_shape'` (i.e., the `numpy.shape` of the image's array) and the `'affine'`, which is simply the affine-transformation that aligns the image with the subject. Usually, it is easiest to provide this information in the form of a template image. For all kinds of subjects (HCP and FreeSurfer), an image is correctly aligned with a subject and thus the subject's cortical surfaces if its affine transfomation correctly aligns it with `subject.images['brain']`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a template image; the new image will have the same shape,\n",
    "# affine, image type, and hader as the template image.\n",
    "template_im = subject.images['brain']\n",
    "# We can use just the template's header for this.\n",
    "template = template_im.header\n",
    "# We can alternately just provide information about the image geometry:\n",
    "#template = {'image_shape': (256,256,256), 'affine': template_im.affine}\n",
    "# Alternately, we can provide an actual image into which the data will\n",
    "# be inserted. In this case, we would want to make a cleared-duplicate\n",
    "# of the brain image (i.e. all voxels set to 0)\n",
    "#template = ny.image_clear(template_im)\n",
    "# All of the above templates should provide the same result.\n",
    "\n",
    "# We are going to save the property from both hemispheres into an image.\n",
    "lh_prop = subject.lh.prop('thickness')\n",
    "rh_prop = subject.rh.prop('thickness')\n",
    "\n",
    "# This may be either 'linear' or 'nearest'; for thickness 'linear'\n",
    "# is probably best, but the difference will be small.\n",
    "method = 'linear'\n",
    "\n",
    "# Do the interpolation. This may take a few minutes the first time it is run.\n",
    "new_im = subject.cortex_to_image((lh_prop, rh_prop), template, method=method,\n",
    "                                 # The template is integer, so we override it.\n",
    "                                 dtype='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have made this new image, let's take a look at it by plotting some slices from it, once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What axis do we want to plot slices along? 0, 1, or 2 (for the first, second,\n",
    "# or third 3D image axis).\n",
    "axis = 2\n",
    "# Which slices along this axis should we plot? These must be at least 0 and at\n",
    "# most 255 (There are 256 slices in each dimension of these images).\n",
    "slices = [75, 125, 175]\n",
    "\n",
    "# Make a figure and axes using matplotlib.pyplot:\n",
    "(fig, axes) = plt.subplots(1, len(slices), figsize=(5, 5/len(slices)), dpi=144)\n",
    "# Plot each of the slices:\n",
    "for (ax, slice_num) in zip(axes, slices):\n",
    "    # Get the slice:\n",
    "    if axis == 0:\n",
    "        imslice = new_im.dataobj[slice_num,:,:]\n",
    "    elif axis == 1:\n",
    "        imslice = new_im.dataobj[:,slice_num,:]\n",
    "    else:\n",
    "        imslice = new_im.dataobj[:,:,slice_num]\n",
    "    ax.imshow(imslice, cmap='hot', vmin=0, vmax=6)\n",
    "    # Turn off labels:\n",
    "    ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image to Cortex Interpolation.** A good test of our interpolation methods is now to ensure that, when we interpolate data from the image we just created back to the cortex, we get approximately the same values. The values we interpolate back out of the volume will not be identical to the volumes we started with because the resolution of the image is finite, but they should be close.\n",
    "\n",
    "The `image_to_cortex()` method of the `Subject` class is capable of interpolating from an image to the cortical surface(s), based on the alignment of the image with the cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lh_prop_interp, rh_prop_interp) = subject.image_to_cortex(new_im, method=method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the hemispheres together to visualize the difference between the original thickenss and the thickness that was interpolated into an image then back onto the cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ny.cortex_plot(subject.lh, surface='midgray',\n",
    "                     color=(lh_prop_interp - lh_prop)**2,\n",
    "                     cmap='hot', vmin=0, vmax=2)\n",
    "fig = ny.cortex_plot(subject.rh, surface='midgray',\n",
    "                     color=(rh_prop_interp - rh_prop)**2,\n",
    "                     cmap='hot', vmin=0, vmax=2,\n",
    "                     figure=fig)\n",
    "\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "## Intersubject Surface Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison between multiple subjects is usually accomplished by first aligning each subject's cortical surface with that of a template surface (*fsaverage* in FreeSurfer, *fs_LR* in the HCP), then interpolating between vertices in the aligned arrangements. The alignment to the template are calculated and saved by FreeSurfer, the HCPpipelines, and various other utilities, but as of when this tutorial was written, `neuropythy` only supports these first two formats. Alignments are calculated by warping the vertices of the subject's spherical (fully inflated) hemisphere in a diffeomorphic fashion with the goal of minimizing the difference between the sulcal topology (curvature and depth) of the subject's vertices and that of the nearby *fsaverage* vertices. The process involves a number of steps, and any who are interested should follow up with the various documentations and papers published by the [FreeSurfer group](https://surfer.nmr.mgh.harvard.edu/).\n",
    "\n",
    "For practical purposes, it is not necessary to understand the details of this algorithm--FreeSurfer is a large complex collection of software that has been under development for decades. However, to better understand what is produced by FreeSurfer's alignment procedure, let us start by looking at its outputs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Subject Registrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "To better understand the various spherical surfaces produced by FreeSurfer, let's start by plotting three spherical surfaces in 3D. The first will be the subject's \"native\" inflated spherical surface. The next will be the subjects \"fsaverage\"-aligned sphere. The last will be The *fsaverage* subject's native sphere.\n",
    "\n",
    "These spheres are accessed not through the `subject.surfaces` dictionary but through the `subject.registrations` dictionary. This is simply a design decision--registrations and surfaces are not fundamentally different except that registrations can be used for interpolation between subjects (more below).\n",
    "\n",
    "Note that you may need to zoom out once the plot has been made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the fsaverage subject.\n",
    "fsaverage = ny.freesurfer_subject('fsaverage')\n",
    "\n",
    "# Get the hemispheres we will be examining.\n",
    "fsa_hemi = fsaverage.hemis[hemi]\n",
    "sub_hemi = subject.hemis[hemi]\n",
    "\n",
    "# Next, get the three registrations we want to plot.\n",
    "sub_native_reg = sub_hemi.registrations['native']\n",
    "sub_fsaverage_reg = sub_hemi.registrations['fsaverage']\n",
    "fsa_native_reg = fsa_hemi.registrations['native']\n",
    "\n",
    "# We want to plot them all three together in one scene, so to do this\n",
    "# we need to translate two of them a bit along the x-axis.\n",
    "sub_native_reg = sub_native_reg.translate([-225,0,0])\n",
    "fsa_native_reg = fsa_native_reg.translate([ 225,0,0])\n",
    "\n",
    "# Now plot them all.\n",
    "fig = ipv.figure(width=900, height=300)\n",
    "ny.cortex_plot(sub_native_reg, figure=fig)\n",
    "ny.cortex_plot(fsa_native_reg, figure=fig)\n",
    "ny.cortex_plot(sub_fsaverage_reg, figure=fig)\n",
    "\n",
    "ipv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate Between Subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Interpolation between subjects requires interpolating between a shared registration. For a subject and the *fsaverage*, this is the subject's *fsaverage*-aligned registration and *fsaverage*'s native. However, for two non-meta subjects, the *fsaverage*-aligned registration of both subjects are used.\n",
    "\n",
    "We will first show how to interpolate from a subject over to the **fsaverage**. This is a very valuable operation to be able to do as it allows you to compute statistics across subejcts of cortical surface data (such as BOLD activation data or source-localized MEG data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The property we're going to interpolate over to fsaverage:\n",
    "sub_prop = sub_hemi.prop('thickness')\n",
    "\n",
    "# The method we use ('nearest' or 'linear'):\n",
    "method = 'linear'\n",
    "\n",
    "# Interpolate the subject's thickness onto the fsaverage surface.\n",
    "fsa_prop = sub_hemi.interpolate(fsa_hemi, sub_prop, method=method)\n",
    "\n",
    "# Let's make a plot of this:\n",
    "ny.cortex_plot(fsa_hemi, surface='inflated',\n",
    "               color=fsa_prop, cmap='hot', vmin=0, vmax=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, for our last exercise, let's interpolate back from the *fsaverage* subject to our subject. It is occasionally nice to be able to plot the *fsaverage*'s average curvature map as an underlay, so let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time we are going to interpolate curvature from the fsaverage\n",
    "# back to the subject. When the property we are interpolating is a\n",
    "# named property of the hemisphere, we can actually just specify it\n",
    "# by name in the interpolation call.\n",
    "fsa_curv_on_sub = fsa_hemi.interpolate(sub_hemi, 'curvature')\n",
    "\n",
    "# We can make a duplicate subject hemisphere with this new property\n",
    "# so that it's easy to plot this curvature map.\n",
    "sub_hemi_fsacurv = sub_hemi.with_prop(curvature=fsa_curv_on_sub)\n",
    "\n",
    "# Great, let's see what this looks like:\n",
    "ny.cortex_plot(sub_hemi_fsacurv, surface='inflated')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Noah's Python Environment",
   "language": "python",
   "name": "python-nben"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
